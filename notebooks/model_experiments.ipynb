{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c85634-03a4-48f6-bdcc-4d9041dc07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from toxigen import label_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c049d046-9917-4cc6-a0fb-80be1f87f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['text', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c1162d5-b4d5-4698-937e-fa46de19cd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt  \" white privilege \" \" is when the gun jam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if you are white in that state watch yourself !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>so the 500 % increase in h1b visas ? yeah i'm ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt antiwhites expose their hatred of white ki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>capitalism  like communism  is an enemy of the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0   rt  \" white privilege \" \" is when the gun jam...      0\n",
       "1    if you are white in that state watch yourself !      0\n",
       "2  so the 500 % increase in h1b visas ? yeah i'm ...      0\n",
       "3   rt antiwhites expose their hatred of white ki...      0\n",
       "4  capitalism  like communism  is an enemy of the...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit_df = pd.read_csv('../data/implicit-hate-corpus/implicit_hate_v1_stg3_posts.tsv', sep=\"\\t\")\n",
    "implicit_df['label'] = 0\n",
    "implicit_df = implicit_df.rename(columns={\"post\": \"text\"})\n",
    "implicit_df_v2 = implicit_df[cols]\n",
    "implicit_df_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "431715b9-457e-440e-bf27-062fbb16df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_56264\\526951477.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alice_df['label'] = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if youre white, you should be proud to be whit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the best thing that could have happened to the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>theres no hope for the future because theres n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if youre white, you should be proud of it.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no matter how much money or power you have, if...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  if youre white, you should be proud to be whit...      1\n",
       "1  the best thing that could have happened to the...      1\n",
       "2  theres no hope for the future because theres n...      1\n",
       "3         if youre white, you should be proud of it.      1\n",
       "4  no matter how much money or power you have, if...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxigen_data = load_dataset(\"toxigen/toxigen-data\", name=\"train\")\n",
    "toxigen_df = pd.DataFrame.from_dict(toxigen_data['train'])\n",
    "alice_df = toxigen_df[toxigen_df['generation_method'] == 'ALICE']\n",
    "\n",
    "alice_df['label'] = 1\n",
    "alice_df_v2 = alice_df.rename(columns={\"generation\": \"text\"})\n",
    "alice_df_v2 = alice_df_v2[cols]\n",
    "alice_df_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a119193-5732-4f4b-b451-1f88ce31d81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6359, 2) (9809, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((16168, 2), (162, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(implicit_df_v2.shape, alice_df_v2.shape)\n",
    "ai_df = pd.concat([implicit_df_v2, alice_df_v2])\n",
    "subai_df = ai_df.sample(frac=.01)\n",
    "ai_df.shape, subai_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d117d62d-f0af-4206-ab09-14e5b820ea8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    104\n",
       "0     58\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subai_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e577d-70d1-46a3-98fa-60fb54149fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.children():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b3fc256-463b-415f-8c70-ccdc3a067cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom HateBERT model\n",
    "class CustomHateBERTModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_classes, dropout_rate=0.3):\n",
    "        super(CustomHateBERTModel, self).__init__()\n",
    "        # Load the pre-trained HateBERT model\n",
    "        self.encoder = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Add custom layers for classification\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through HateBERT\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = encoder_outputs.last_hidden_state\n",
    "        pooled_output = self.pooling(last_hidden_state.permute(0, 2, 1)).squeeze(-1)\n",
    "\n",
    "        # Apply dropout and dense layer\n",
    "        dropped_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(dropped_output)\n",
    "        return logits\n",
    "\n",
    "# # Initialize the model\n",
    "# pretrained_model_name = \"GroNLP/hateBERT\"\n",
    "# num_classes = 3  # Replace with your number of classes\n",
    "# model = CustomHateBERTModel(pretrained_model_name, num_classes)\n",
    "\n",
    "# # Loss function: Categorical Cross-Entropy\n",
    "# criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "\n",
    "# # Optimizer: Adam with learning rate 0.001\n",
    "# optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Metric: Accuracy function\n",
    "def accuracy(preds, labels):\n",
    "    _, predictions = torch.max(preds, dim=1)\n",
    "    return (predictions == labels).sum().item() / labels.size(0)\n",
    "\n",
    "# Example Dataset (for demonstration purposes)\n",
    "class ExampleDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, data_loader, optimizer, criterion, device):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        acc = accuracy(outputs, labels)\n",
    "        total_acc += acc\n",
    "\n",
    "    return total_loss / len(data_loader), total_acc / len(data_loader)\n",
    "\n",
    "# Example usage\n",
    "# tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "# texts = [\"This is a sample text.\", \"Another example of text.\"]\n",
    "# labels = [0, 1]  # Example labels (multi-class indices)\n",
    "\n",
    "# dataset = ExampleDataset(texts, labels, tokenizer, max_len=128)\n",
    "# data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# # Training configuration\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# epochs = 3\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     train_loss, train_acc = train_model(model, data_loader, optimizer, criterion, device)\n",
    "#     print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "#     print(f\"Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "776b0ea2-779c-4b43-8894-fc00a711dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for text classification using a Hugging Face tokenizer.\n",
    "\n",
    "    Attributes:\n",
    "        texts (list): A list of input text strings.\n",
    "        labels (list): A list of integer labels corresponding to the input texts.\n",
    "        tokenizer (AutoTokenizer): Tokenizer from the Hugging Face Transformers library.\n",
    "        max_len (int): Maximum length of tokenized input sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        \"\"\"Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            texts (list): A list of strings containing the input texts.\n",
    "            labels (list): A list of strings or integers containing the labels.\n",
    "            tokenizer (AutoTokenizer): Tokenizer from the Hugging Face Transformers library.\n",
    "            max_len (int): Maximum length of tokenized input sequences.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Fetches a single data point at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the data point to fetch.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing input IDs, attention masks, and the label.\n",
    "        \"\"\"\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def create_text_classification_dataset(df, \n",
    "                                       #label_mapping,\n",
    "                                       tokenizer_name, max_len):\n",
    "    \"\"\"Creates a PyTorch dataset from a pandas dataframe for text classification.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing two columns: the first column has the text, and the second column has the labels.\n",
    "        label_mapping (dict): A dictionary mapping string labels to integer class indices.\n",
    "        tokenizer_name (str): Name of the pre-trained tokenizer (e.g., \"GroNLP/hateBERT\").\n",
    "        max_len (int): Maximum length of tokenized input sequences.\n",
    "\n",
    "    Returns:\n",
    "        TextClassificationDataset: A PyTorch dataset ready for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract texts and map string labels to integers\n",
    "    texts = df.iloc[:, 0].tolist()\n",
    "    # labels = df.iloc[:, 1].map(label_mapping).tolist()\n",
    "    labels = df.iloc[:, 1].tolist()\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # Create and return the dataset\n",
    "    return TextClassificationDataset(texts, labels, tokenizer, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd35e7-a638-4b6e-aa9e-28f8144d5928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f5460-2917-4cae-9b0a-3e05dbc725c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa83b3-610f-4c03-812d-9dc7524fef05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e3d145-27dc-44e9-a066-b916bd8f844a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184601b5-1905-41cc-bd92-c00d52896bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b26cae15-396e-4363-beca-1f3d83125cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\.conda\\envs\\ai-hate\\Lib\\site-packages\\transformers\\modeling_utils.py:1435: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at GroNLP/hateBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "pretrained_model_name = \"GroNLP/hateBERT\"\n",
    "num_classes = 2  # Replace with your number of classes\n",
    "model = CustomHateBERTModel(pretrained_model_name, num_classes)\n",
    "\n",
    "# Loss function: Categorical Cross-Entropy\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "\n",
    "# Optimizer: Adam with learning rate 0.001\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89938120-6c4d-4b49-8f11-be387577bd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "text_ds = create_text_classification_dataset(subai_df, \n",
    "                                             #label_mapper, \n",
    "                                             tokenizer_name=\"GroNLP/hateBERT\", max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8da49c0-dc73-4c88-963f-a482553f3319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Loss: 0.6989, Accuracy: 0.5494\n",
      "Epoch 2/3\n",
      "Loss: 0.6703, Accuracy: 0.6420\n",
      "Epoch 3/3\n",
      "Loss: 0.6799, Accuracy: 0.6296\n"
     ]
    }
   ],
   "source": [
    "#dataset = ExampleDataset(texts, labels, tokenizer, max_len=128)\n",
    "data_loader = DataLoader(text_ds, batch_size=2, shuffle=True)\n",
    "\n",
    "# Training configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_model(model, data_loader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c721fd5-db06-4f75-ae61-5816ed366226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-hate",
   "language": "python",
   "name": "ai-hate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
