{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c85634-03a4-48f6-bdcc-4d9041dc07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from toxigen import label_annotations\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049d046-9917-4cc6-a0fb-80be1f87f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['text', 'label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1162d5-b4d5-4698-937e-fa46de19cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit_df = pd.read_csv('../data/implicit-hate-corpus/implicit_hate_v1_stg1_posts.tsv', sep=\"\\t\")\n",
    "implicit_df_v2 = implicit_df[implicit_df['class'] == 'implicit_hate']\n",
    "implicit_df_v3 = implicit_df_v2.rename(columns={\"post\": \"text\"})\n",
    "implicit_df_v4 = implicit_df_v3[['text']]\n",
    "implicit_df_v4['label'] = 0\n",
    "print(implicit_df_v4.shape)\n",
    "implicit_df_v4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d755fa7-02a4-480f-8f89-fe02ab49727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431715b9-457e-440e-bf27-062fbb16df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxigen_data = load_dataset(\"toxigen/toxigen-data\", name=\"train\")\n",
    "toxigen_df = pd.DataFrame.from_dict(toxigen_data['train'])\n",
    "alice_df = toxigen_df[toxigen_df['generation_method'] == 'ALICE']\n",
    "alice_hate_df = alice_df[alice_df['prompt_label'] == 1]\n",
    "#alice_df['label'] = 1\n",
    "alice_df_v2 = alice_hate_df.rename(columns={\"generation\": \"text\", 'prompt_label': 'label'})\n",
    "alice_df_v2 = alice_df_v2[cols]\n",
    "alice_df_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329b84a-8a1a-4abe-9d2f-a23ef4e1911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toxigen_df.columns)\n",
    "toxigen_df['prompt_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a119193-5732-4f4b-b451-1f88ce31d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(implicit_df_v2.shape, alice_df_v2.shape)\n",
    "ai_df = pd.concat([implicit_df_v4, alice_df_v2])\n",
    "subai_df = ai_df.sample(frac=.005)\n",
    "ai_df.shape, subai_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117d62d-f0af-4206-ab09-14e5b820ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subai_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e577d-70d1-46a3-98fa-60fb54149fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.children():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3fc256-463b-415f-8c70-ccdc3a067cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom HateBERT model\n",
    "class CustomHateBERTModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_classes, dropout_rate=0.3):\n",
    "        super(CustomHateBERTModel, self).__init__()\n",
    "        # Load the pre-trained HateBERT model\n",
    "        self.encoder = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Add custom layers for classification\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through HateBERT\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = encoder_outputs.last_hidden_state\n",
    "        pooled_output = self.pooling(last_hidden_state.permute(0, 2, 1)).squeeze(-1)\n",
    "\n",
    "        # Apply dropout and dense layer\n",
    "        dropped_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(dropped_output)\n",
    "        return logits\n",
    "\n",
    "# # Initialize the model\n",
    "# pretrained_model_name = \"GroNLP/hateBERT\"\n",
    "# num_classes = 3  # Replace with your number of classes\n",
    "# model = CustomHateBERTModel(pretrained_model_name, num_classes)\n",
    "\n",
    "# # Loss function: Categorical Cross-Entropy\n",
    "# criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "\n",
    "# # Optimizer: Adam with learning rate 0.001\n",
    "# optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Metric: Accuracy function\n",
    "def accuracy(preds, labels):\n",
    "    _, predictions = torch.max(preds, dim=1)\n",
    "    return (predictions == labels).sum().item() / labels.size(0)\n",
    "\n",
    "def f1_score_metric(y_true, y_pred, average='weighted'):\n",
    "    \"\"\"Calculates the F1 score for predictions and true labels.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): True labels.\n",
    "        y_pred (torch.Tensor): Predicted probabilities or logits.\n",
    "\n",
    "    Returns:\n",
    "        float: The F1 score.\n",
    "    \"\"\"\n",
    "    y_pred_classes = torch.argmax(y_pred, dim=1).cpu().numpy()\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    return f1_score(y_true, y_pred_classes, average=average)\n",
    "\n",
    "# Example Dataset (for demonstration purposes)\n",
    "class ExampleDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, data_loader, optimizer, criterion, device):\n",
    "    \"\"\"Training loop for a PyTorch model, including accuracy and F1 score computation.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to train.\n",
    "        data_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        device (torch.device): Device to train on (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average loss, accuracy, and F1 score for the epoch.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        acc = (torch.argmax(outputs, dim=1) == labels).sum().item() / labels.size(0)\n",
    "        total_acc += acc\n",
    "\n",
    "        # Compute F1 score\n",
    "        f1 = f1_score_metric(labels, outputs)\n",
    "        total_f1 += f1\n",
    "\n",
    "    return total_loss / len(data_loader), total_acc / len(data_loader), total_f1 / len(data_loader)\n",
    "\n",
    "# Example usage\n",
    "# tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "# texts = [\"This is a sample text.\", \"Another example of text.\"]\n",
    "# labels = [0, 1]  # Example labels (multi-class indices)\n",
    "\n",
    "# dataset = ExampleDataset(texts, labels, tokenizer, max_len=128)\n",
    "# data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# # Training configuration\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# epochs = 3\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     train_loss, train_acc = train_model(model, data_loader, optimizer, criterion, device)\n",
    "#     print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "#     print(f\"Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b0ea2-779c-4b43-8894-fc00a711dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for text classification using a Hugging Face tokenizer.\n",
    "\n",
    "    Attributes:\n",
    "        texts (list): A list of input text strings.\n",
    "        labels (list): A list of integer labels corresponding to the input texts.\n",
    "        tokenizer (AutoTokenizer): Tokenizer from the Hugging Face Transformers library.\n",
    "        max_len (int): Maximum length of tokenized input sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        \"\"\"Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            texts (list): A list of strings containing the input texts.\n",
    "            labels (list): A list of strings or integers containing the labels.\n",
    "            tokenizer (AutoTokenizer): Tokenizer from the Hugging Face Transformers library.\n",
    "            max_len (int): Maximum length of tokenized input sequences.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Fetches a single data point at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the data point to fetch.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing input IDs, attention masks, and the label.\n",
    "        \"\"\"\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def create_text_classification_dataset(df, \n",
    "                                       #label_mapping,\n",
    "                                       tokenizer_name, max_len):\n",
    "    \"\"\"Creates a PyTorch dataset from a pandas dataframe for text classification.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing two columns: the first column has the text, and the second column has the labels.\n",
    "        label_mapping (dict): A dictionary mapping string labels to integer class indices.\n",
    "        tokenizer_name (str): Name of the pre-trained tokenizer (e.g., \"GroNLP/hateBERT\").\n",
    "        max_len (int): Maximum length of tokenized input sequences.\n",
    "\n",
    "    Returns:\n",
    "        TextClassificationDataset: A PyTorch dataset ready for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract texts and map string labels to integers\n",
    "    texts = df.iloc[:, 0].tolist()\n",
    "    # labels = df.iloc[:, 1].map(label_mapping).tolist()\n",
    "    labels = df.iloc[:, 1].tolist()\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # Create and return the dataset\n",
    "    return TextClassificationDataset(texts, labels, tokenizer, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd35e7-a638-4b6e-aa9e-28f8144d5928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f5460-2917-4cae-9b0a-3e05dbc725c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa83b3-610f-4c03-812d-9dc7524fef05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e3d145-27dc-44e9-a066-b916bd8f844a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184601b5-1905-41cc-bd92-c00d52896bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26cae15-396e-4363-beca-1f3d83125cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "pretrained_model_name = \"GroNLP/hateBERT\"\n",
    "num_classes = 2  # Replace with your number of classes\n",
    "model = CustomHateBERTModel(pretrained_model_name, num_classes)\n",
    "\n",
    "# Loss function: Categorical Cross-Entropy\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "\n",
    "# Optimizer: Adam with learning rate 0.001\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27480108-fab7-42e8-b23f-6b5b7c3e029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subai_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89938120-6c4d-4b49-8f11-be387577bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = create_text_classification_dataset(subai_df, \n",
    "                                             #label_mapper, \n",
    "                                             tokenizer_name=\"GroNLP/hateBERT\", max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da49c0-dc73-4c88-963f-a482553f3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = ExampleDataset(texts, labels, tokenizer, max_len=128)\n",
    "data_loader = DataLoader(text_ds, batch_size=2, shuffle=True)\n",
    "\n",
    "# Training configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc, train_f1score = train_model(model, data_loader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, F1-Score: {train_f1score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c721fd5-db06-4f75-ae61-5816ed366226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e3c505-a6f2-4a3f-a224-4409f87e7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train HateBERT\"\"\"\n",
    "\n",
    "#import os\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "#from toxigen import label_annotations\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "#from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def create_binary_dataset():\n",
    "    \"\"\"Imports the Implicit Hate and Toxigen datasets. Labels all Implicit Hate data as 0 for\n",
    "    human generated and all data from Toxigen as 1 for AI generated. Only the implicit hate\n",
    "    generated by the ALICE technique is used. \n",
    "    \n",
    "    Returns:\n",
    "        ai_df (pd.DataFrame): Human and AI generated hate\n",
    "    \"\"\"\n",
    "    # load implicit hate dataset\n",
    "    cols = ['text', 'label']\n",
    "    implicit_df = pd.read_csv('../data/implicit-hate-corpus/implicit_hate_v1_stg1_posts.tsv', sep=\"\\t\")\n",
    "    implicit_df_v2 = implicit_df[implicit_df['class'] == 'implicit_hate']\n",
    "    implicit_df_v3 = implicit_df_v2.rename(columns={\"post\": \"text\"})\n",
    "    implicit_df_v4 = implicit_df_v3[['text']]\n",
    "    implicit_df_v4['label'] = 0\n",
    "    \n",
    "    implicit_df_v4['label'] = implicit_df_v4['label'].astype(int)\n",
    "    implicit_df_v4['text'] = implicit_df_v4['text'].astype(str)\n",
    "    implicit_df_v4 = implicit_df_v4[cols]\n",
    "    \n",
    "    # load toxigen\n",
    "    toxigen_data = load_dataset(\"toxigen/toxigen-data\", name=\"train\")\n",
    "    toxigen_df = pd.DataFrame.from_dict(toxigen_data['train'])\n",
    "    alice_df = toxigen_df[toxigen_df['generation_method'] == 'ALICE']\n",
    "\n",
    "    alice_hate_df = alice_df[alice_df['prompt_label'] == 1]\n",
    "    alice_df_v2 = alice_hate_df.rename(columns={\"generation\": \"text\", 'prompt_label': 'label'})\n",
    "\n",
    "    alice_df_v2['label'] = alice_df_v2['label'].astype(int)\n",
    "    alice_df_v2['text'] = alice_df_v2['text'].astype(str)\n",
    "\n",
    "    alice_df_v2 = alice_df_v2[cols]\n",
    "\n",
    "    # binary dataset\n",
    "    ai_df = pd.concat([implicit_df_v4, alice_df_v2])\n",
    "    return ai_df\n",
    "\n",
    "class CustomHateBERTModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_classes, dropout_rate=0.3):\n",
    "        super(CustomHateBERTModel, self).__init__()\n",
    "        # Load the pre-trained HateBERT model\n",
    "        self.encoder = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Add custom layers for classification\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through HateBERT\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = encoder_outputs.last_hidden_state\n",
    "        pooled_output = self.pooling(last_hidden_state.permute(0, 2, 1)).squeeze(-1)\n",
    "\n",
    "        # Apply dropout and dense layer\n",
    "        dropped_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(dropped_output)\n",
    "        return logits\n",
    "    \n",
    "# Metric: Accuracy function\n",
    "def accuracy(preds, labels):\n",
    "    _, predictions = torch.max(preds, dim=1)\n",
    "    return (predictions == labels).sum().item() / labels.size(0)\n",
    "\n",
    "def f1_score_metric(y_true, y_pred, average='weighted'):\n",
    "    \"\"\"Calculates the F1 score for predictions and true labels.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): True labels.\n",
    "        y_pred (torch.Tensor): Predicted probabilities or logits.\n",
    "\n",
    "    Returns:\n",
    "        float: The F1 score.\n",
    "    \"\"\"\n",
    "    y_pred_classes = torch.argmax(y_pred, dim=1).cpu().numpy()\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    return f1_score(y_true, y_pred_classes, average=average)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, data_loader, optimizer, criterion, device):\n",
    "    \"\"\"Training loop for a PyTorch model, including accuracy and F1 score computation.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to train.\n",
    "        data_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        device (torch.device): Device to train on (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average loss, accuracy, and F1 score for the epoch.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        acc = (torch.argmax(outputs, dim=1) == labels).sum().item() / labels.size(0)\n",
    "        total_acc += acc\n",
    "\n",
    "        # Compute F1 score\n",
    "        f1 = f1_score_metric(labels, outputs)\n",
    "        total_f1 += f1\n",
    "\n",
    "    return total_loss / len(data_loader), total_acc / len(data_loader), total_f1 / len(data_loader)\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for text classification using a Hugging Face tokenizer.\n",
    "\n",
    "    Attributes:\n",
    "        texts (list): A list of input text strings.\n",
    "        labels (list): A list of integer labels corresponding to the input texts.\n",
    "        tokenizer (AutoTokenizer): Tokenizer from the Hugging Face Transformers library.\n",
    "        max_len (int): Maximum length of tokenized input sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        \"\"\"Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            texts (list): A list of strings containing the input texts.\n",
    "            labels (list): A list of strings or integers containing the labels.\n",
    "            tokenizer (AutoTokenizer): Tokenizer from the Hugging Face Transformers library.\n",
    "            max_len (int): Maximum length of tokenized input sequences.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Fetches a single data point at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the data point to fetch.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing input IDs, attention masks, and the label.\n",
    "        \"\"\"\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def create_text_classification_dataset(df, tokenizer_name, max_len):\n",
    "    \"\"\"Creates a PyTorch dataset from a pandas dataframe for text classification.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing two columns: the first column has the text, and the second column has the labels.\n",
    "        label_mapping (dict): A dictionary mapping string labels to integer class indices.\n",
    "        tokenizer_name (str): Name of the pre-trained tokenizer (e.g., \"GroNLP/hateBERT\").\n",
    "        max_len (int): Maximum length of tokenized input sequences.\n",
    "\n",
    "    Returns:\n",
    "        TextClassificationDataset: A PyTorch dataset ready for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract texts and map string labels to integers\n",
    "    texts = df.iloc[:, 0].tolist()\n",
    "    labels = df.iloc[:, 1].tolist()\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # Create and return the dataset\n",
    "    return TextClassificationDataset(texts, labels, tokenizer, max_len)\n",
    "\n",
    "def main(sub_df=False):\n",
    "    \"\"\"Train a binary model\"\"\"\n",
    "    # configs\n",
    "    FRAC = 0.005\n",
    "    MAX_LEN = 128\n",
    "    LEARNING_RATE = 0.001\n",
    "    BATCH_SIZE = 2\n",
    "    EPOCHS = 3\n",
    "    pretrained_model_name = \"GroNLP/hateBERT\"\n",
    "    NUM_CLASSES = 2\n",
    "    model = CustomHateBERTModel(pretrained_model_name, NUM_CLASSES)\n",
    "\n",
    "    # Loss function: Categorical Cross-Entropy\n",
    "    criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "\n",
    "    # Optimizer: Adam with learning rate 0.001\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(\"Creating dataset\")\n",
    "    ai_df = create_binary_dataset()\n",
    "    if sub_df:\n",
    "        ai_df = ai_df.sample(frac=FRAC)\n",
    "\n",
    "    print(\"creating pytorch dataset\")\n",
    "    text_ds = create_text_classification_dataset(ai_df,\n",
    "                                                tokenizer_name=pretrained_model_name,\n",
    "                                                max_len=MAX_LEN)\n",
    "\n",
    "    print(\"creating data loader\")\n",
    "    data_loader = DataLoader(text_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Training configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(\"Starting training loop\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1score = train_model(model, data_loader, optimizer, criterion, device)\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        print(f\"Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, F1-Score: {train_f1score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sub_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d78a2-c998-4400-b3c8-d8a62ad0b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, save_dir=\"checkpoints\", filename=None):\n",
    "    \"\"\"Saves a model checkpoint.\"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    if filename is None:\n",
    "        filename = f\"model_epoch_{epoch}.pt\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }, save_path)\n",
    "    print(f\"Checkpoint saved at: {save_path}\")\n",
    "\n",
    "def main(sub_df=False):\n",
    "    \"\"\"Train a binary model with checkpointing and metrics logging.\"\"\"\n",
    "    # Configurations\n",
    "    FRAC = 0.005\n",
    "    MAX_LEN = 128\n",
    "    LEARNING_RATE = 0.001\n",
    "    BATCH_SIZE = 2\n",
    "    EPOCHS = 3\n",
    "    CHECKPOINT_INTERVAL = 2  # Save checkpoint every 2 epochs\n",
    "    pretrained_model_name = \"GroNLP/hateBERT\"\n",
    "    NUM_CLASSES = 2\n",
    "    CHECKPOINT_DIR = \"checkpoints\"\n",
    "    LOG_DIR = \"logs\"\n",
    "\n",
    "    model = CustomHateBERTModel(pretrained_model_name, NUM_CLASSES)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(\"Creating dataset\")\n",
    "    ai_df = create_binary_dataset()\n",
    "    if sub_df:\n",
    "        ai_df = ai_df.sample(frac=FRAC)\n",
    "\n",
    "    print(\"Creating PyTorch dataset\")\n",
    "    text_ds = create_text_classification_dataset(\n",
    "        ai_df,\n",
    "        tokenizer_name=pretrained_model_name,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    print(\"Creating data loader\")\n",
    "    data_loader = DataLoader(text_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Training configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "    print(\"Starting training loop\")\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, train_acc, train_f1score = train_model(\n",
    "            model, data_loader, optimizer, criterion, device\n",
    "        )\n",
    "        print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        print(f\"Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, F1-Score: {train_f1score:.4f}\")\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "        writer.add_scalar(\"F1-Score/train\", train_f1score, epoch)\n",
    "\n",
    "        # Save checkpoints every CHECKPOINT_INTERVAL epochs\n",
    "        if epoch % CHECKPOINT_INTERVAL == 0:\n",
    "            save_checkpoint(model, optimizer, epoch, save_dir=CHECKPOINT_DIR)\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sub_df=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b436b9df-f407-4957-a42a-40a7e9751d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c57e3c0-bf49-4b65-b3e5-e819f3c7cb98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5de5f-4ee7-4b5f-8b8f-d76c16dc4c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-hate",
   "language": "python",
   "name": "ai-hate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
